{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref: https://data-flair.training/blogs/python-mini-project-speech-emotion-recognition/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: librosa in c:\\users\\stratus-research\\anaconda3\\lib\\site-packages (0.8.0)Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: soundfile in c:\\users\\stratus-research\\anaconda3\\lib\\site-packages (0.10.3.post1)\n",
      "Requirement already satisfied: numpy in c:\\users\\stratus-research\\anaconda3\\lib\\site-packages (1.19.2)\n",
      "Requirement already satisfied: sklearn in c:\\users\\stratus-research\\anaconda3\\lib\\site-packages (0.0)\n",
      "Collecting pyaudio\n",
      "  Using cached PyAudio-0.2.11.tar.gz (37 kB)\n",
      "Requirement already satisfied: decorator>=3.0.0 in c:\\users\\stratus-research\\anaconda3\\lib\\site-packages (from librosa) (4.4.2)\n",
      "Requirement already satisfied: numba>=0.43.0 in c:\\users\\stratus-research\\anaconda3\\lib\\site-packages (from librosa) (0.51.2)\n",
      "Requirement already satisfied: scipy>=1.0.0 in c:\\users\\stratus-research\\anaconda3\\lib\\site-packages (from librosa) (1.5.2)\n",
      "Requirement already satisfied: audioread>=2.0.0 in c:\\users\\stratus-research\\anaconda3\\lib\\site-packages (from librosa) (2.1.9)\n",
      "Requirement already satisfied: joblib>=0.14 in c:\\users\\stratus-research\\anaconda3\\lib\\site-packages (from librosa) (0.17.0)\n",
      "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in c:\\users\\stratus-research\\anaconda3\\lib\\site-packages (from librosa) (0.23.2)\n",
      "Requirement already satisfied: pooch>=1.0 in c:\\users\\stratus-research\\anaconda3\\lib\\site-packages (from librosa) (1.3.0)\n",
      "Requirement already satisfied: resampy>=0.2.2 in c:\\users\\stratus-research\\anaconda3\\lib\\site-packages (from librosa) (0.2.2)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\stratus-research\\anaconda3\\lib\\site-packages (from soundfile) (1.14.3)\n",
      "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in c:\\users\\stratus-research\\anaconda3\\lib\\site-packages (from numba>=0.43.0->librosa) (0.34.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\stratus-research\\anaconda3\\lib\\site-packages (from numba>=0.43.0->librosa) (50.3.1.post20201107)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\stratus-research\\anaconda3\\lib\\site-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa) (2.1.0)\n",
      "Requirement already satisfied: requests in c:\\users\\stratus-research\\anaconda3\\lib\\site-packages (from pooch>=1.0->librosa) (2.24.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\stratus-research\\anaconda3\\lib\\site-packages (from pooch>=1.0->librosa) (20.4)\n",
      "Requirement already satisfied: appdirs in c:\\users\\stratus-research\\anaconda3\\lib\\site-packages (from pooch>=1.0->librosa) (1.4.4)\n",
      "Requirement already satisfied: six>=1.3 in c:\\users\\stratus-research\\anaconda3\\lib\\site-packages (from resampy>=0.2.2->librosa) (1.15.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\stratus-research\\anaconda3\\lib\\site-packages (from cffi>=1.0->soundfile) (2.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\stratus-research\\anaconda3\\lib\\site-packages (from requests->pooch>=1.0->librosa) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\stratus-research\\anaconda3\\lib\\site-packages (from requests->pooch>=1.0->librosa) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\stratus-research\\anaconda3\\lib\\site-packages (from requests->pooch>=1.0->librosa) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\stratus-research\\anaconda3\\lib\\site-packages (from requests->pooch>=1.0->librosa) (3.0.4)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\stratus-research\\anaconda3\\lib\\site-packages (from packaging->pooch>=1.0->librosa) (2.4.7)\n",
      "Building wheels for collected packages: pyaudio\n",
      "  Building wheel for pyaudio (setup.py): started\n",
      "  Building wheel for pyaudio (setup.py): finished with status 'error'\n",
      "\n",
      "  Running setup.py clean for pyaudio\n",
      "Failed to build pyaudio\n",
      "Installing collected packages: pyaudio\n",
      "    Running setup.py install for pyaudio: started\n",
      "    Running setup.py install for pyaudio: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ERROR: Command errored out with exit status 1:\n",
      "   command: 'C:\\Users\\STRATUS-Research\\anaconda3\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\STRATUS-Research\\\\AppData\\\\Local\\\\Temp\\\\pip-install-e9o8g57k\\\\pyaudio\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\STRATUS-Research\\\\AppData\\\\Local\\\\Temp\\\\pip-install-e9o8g57k\\\\pyaudio\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d 'C:\\Users\\STRATUS-Research\\AppData\\Local\\Temp\\pip-wheel-eqwdqlst'\n",
      "       cwd: C:\\Users\\STRATUS-Research\\AppData\\Local\\Temp\\pip-install-e9o8g57k\\pyaudio\\\n",
      "  Complete output (9 lines):\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-3.8\n",
      "  copying src\\pyaudio.py -> build\\lib.win-amd64-3.8\n",
      "  running build_ext\n",
      "  building '_portaudio' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  ----------------------------------------\n",
      "  ERROR: Failed building wheel for pyaudio\n",
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'C:\\Users\\STRATUS-Research\\anaconda3\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\STRATUS-Research\\\\AppData\\\\Local\\\\Temp\\\\pip-install-e9o8g57k\\\\pyaudio\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\STRATUS-Research\\\\AppData\\\\Local\\\\Temp\\\\pip-install-e9o8g57k\\\\pyaudio\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\STRATUS-Research\\AppData\\Local\\Temp\\pip-record-lmnwe01q\\install-record.txt' --single-version-externally-managed --compile --install-headers 'C:\\Users\\STRATUS-Research\\anaconda3\\Include\\pyaudio'\n",
      "         cwd: C:\\Users\\STRATUS-Research\\AppData\\Local\\Temp\\pip-install-e9o8g57k\\pyaudio\\\n",
      "    Complete output (9 lines):\n",
      "    running install\n",
      "    running build\n",
      "    running build_py\n",
      "    creating build\n",
      "    creating build\\lib.win-amd64-3.8\n",
      "    copying src\\pyaudio.py -> build\\lib.win-amd64-3.8\n",
      "    running build_ext\n",
      "    building '_portaudio' extension\n",
      "    error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "    ----------------------------------------\n",
      "ERROR: Command errored out with exit status 1: 'C:\\Users\\STRATUS-Research\\anaconda3\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\STRATUS-Research\\\\AppData\\\\Local\\\\Temp\\\\pip-install-e9o8g57k\\\\pyaudio\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\STRATUS-Research\\\\AppData\\\\Local\\\\Temp\\\\pip-install-e9o8g57k\\\\pyaudio\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\STRATUS-Research\\AppData\\Local\\Temp\\pip-record-lmnwe01q\\install-record.txt' --single-version-externally-managed --compile --install-headers 'C:\\Users\\STRATUS-Research\\anaconda3\\Include\\pyaudio' Check the logs for full command output.\n"
     ]
    }
   ],
   "source": [
    "pip install librosa soundfile numpy sklearn pyaudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import soundfile\n",
    "import os, glob, pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Define a function extract_feature to extract the mfcc, chroma, and mel features from a sound file. This function takes 4 parameters- the file name and three Boolean parameters for the three features:\n",
    "\n",
    "mfcc: Mel Frequency Cepstral Coefficient, represents the short-term power spectrum of a sound\n",
    "chroma: Pertains to the 12 different pitch classes\n",
    "mel: Mel Spectrogram Frequency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 1\n",
    "Open the sound file with soundfile.SoundFile using with-as so it’s automatically closed once we’re done. Read from it and call it X. Also, get the sample rate. If chroma is True, get the Short-Time Fourier Transform of X.\n",
    "\n",
    "Let result be an empty numpy array. Now, for each feature of the three, if it exists, make a call to the corresponding function from librosa.feature (eg- librosa.feature.mfcc for mfcc), and get the mean value. Call the function hstack() from numpy with result and the feature value, and store this in result. hstack() stacks arrays in sequence horizontally (in a columnar fashion). Then, return the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(file_name, mfcc, chroma, mel):\n",
    "    with soundfile.SoundFile(file_name) as sound_file:\n",
    "        X = sound_file.read(dtype=\"float32\")\n",
    "        sample_rate=sound_file.samplerate\n",
    "        if chroma:\n",
    "            stft=np.abs(librosa.stft(X))\n",
    "        result=np.array([])\n",
    "        if mfcc:\n",
    "            mfccs=np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
    "            result=np.hstack((result, mfccs))\n",
    "        if chroma:\n",
    "            chroma=np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n",
    "            result=np.hstack((result, chroma))\n",
    "        if mel:\n",
    "            mel=np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
    "            result=np.hstack((result, mel))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Now, let’s define a dictionary to hold numbers and the emotions available in the RAVDESS dataset, and a list to hold those we want- calm, happy, fearful, disgust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'01': 'neutral',\n",
       " '02': 'calm',\n",
       " '03': 'happy',\n",
       " '04': 'sad',\n",
       " '05': 'angry',\n",
       " '06': 'fearful',\n",
       " '07': 'disgust',\n",
       " '08': 'surprised'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DataFlair - Emotions in the RAVDESS dataset\n",
    "emotions={\n",
    "  '01':'neutral',\n",
    "  '02':'calm',\n",
    "  '03':'happy',\n",
    "  '04':'sad',\n",
    "  '05':'angry',\n",
    "  '06':'fearful',\n",
    "  '07':'disgust',\n",
    "  '08':'surprised'\n",
    "}\n",
    "\n",
    "#DataFlair - Emotions to observe\n",
    "observed_emotions=['calm', 'happy', 'fearful', 'disgust']\n",
    "emotions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Now, let’s load the data with a function load_data() – this takes in the relative size of the test set as parameter. x and y are empty lists; we’ll use the glob() function from the glob module to get all the pathnames for the sound files in our dataset. The pattern we use for this is: “D:\\\\DataFlair\\\\ravdess data\\\\Actor_*\\\\*.wav”. This is because our dataset looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, for each such path, get the basename of the file, the emotion by splitting the name around ‘-’ and extracting the third value:\n",
    "Using our emotions dictionary, this number is turned into an emotion, and our function checks whether this emotion is in our list of observed_emotions; if not, it continues to the next file. It makes a call to extract_feature and stores what is returned in ‘feature’. Then, it appends the feature to x and the emotion to y. So, the list x holds the features and y holds the emotions. We call the function train_test_split with these, the test size, and a random state value, and return that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataFlair - Load the data and extract features for each sound file\n",
    "def load_data(test_size=0.2):\n",
    "    x,y=[],[]\n",
    "    for file in glob.glob(\"data\\\\Actor_*\\\\*.wav\"):\n",
    "        file_name=os.path.basename(file)\n",
    "        emotion=emotions[file_name.split(\"-\")[2]]\n",
    "        if emotion not in observed_emotions:\n",
    "            continue\n",
    "        feature=extract_feature(file, mfcc=True, chroma=True, mel=True)\n",
    "        x.append(feature)\n",
    "        y.append(emotion)\n",
    "    return train_test_split(np.array(x), y, test_size=test_size, random_state=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test= load_data(.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Observe the shape of the training and testing datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      " [[-5.21694946e+02  5.43121452e+01 -1.12595196e+01 ...  2.16407017e-04\n",
      "   1.40978998e-04  1.14794449e-04]\n",
      " [-4.52471008e+02  1.81078854e+01 -2.65877132e+01 ...  5.50175435e-04\n",
      "   5.70777862e-04  4.87014215e-04]\n",
      " [-5.58079468e+02  5.66460762e+01  4.79677868e+00 ...  2.85093585e-04\n",
      "   1.95071072e-04  1.24067476e-04]\n",
      " ...\n",
      " [-6.00510132e+02  5.31514320e+01  8.95492458e+00 ...  2.66101619e-04\n",
      "   1.20797216e-04  9.05639026e-05]\n",
      " [-5.49937622e+02  2.05582409e+01 -1.82497749e+01 ...  2.15354215e-04\n",
      "   1.42610632e-04  5.24237585e-05]\n",
      " [-6.91638672e+02  7.27555847e+01  5.44958353e+00 ...  1.47388891e-05\n",
      "   6.60104979e-06  2.86574937e-06]] \n",
      "test\n",
      " [[-5.57380188e+02  5.61003227e+01 -8.54372311e+00 ...  5.85538401e-05\n",
      "   3.56250130e-05  2.51964248e-05]\n",
      " [-4.99206329e+02  4.36633873e+00 -3.76205444e+01 ...  1.71078209e-04\n",
      "   1.43228433e-04  9.51329712e-05]\n",
      " [-5.68905151e+02  3.63499107e+01 -5.40141392e+00 ...  6.29053276e-04\n",
      "   2.47150223e-04  1.85339581e-04]\n",
      " ...\n",
      " [-6.47894592e+02  4.48749809e+01 -2.81135511e+00 ...  4.23921811e-05\n",
      "   4.01656107e-05  1.77828188e-05]\n",
      " [-4.89697357e+02  5.44996033e+01 -1.18616142e+01 ...  1.43240066e-03\n",
      "   1.24061503e-03  6.21302519e-04]\n",
      " [-5.86724182e+02  4.80303535e+01  9.68061829e+00 ...  4.16368828e-04\n",
      "   2.13918756e-04  9.65486106e-05]]\n",
      "[[-5.21694946e+02  5.43121452e+01 -1.12595196e+01 ...  2.16407017e-04\n",
      "   1.40978998e-04  1.14794449e-04]\n",
      " [-4.52471008e+02  1.81078854e+01 -2.65877132e+01 ...  5.50175435e-04\n",
      "   5.70777862e-04  4.87014215e-04]\n",
      " [-5.58079468e+02  5.66460762e+01  4.79677868e+00 ...  2.85093585e-04\n",
      "   1.95071072e-04  1.24067476e-04]\n",
      " ...\n",
      " [-6.00510132e+02  5.31514320e+01  8.95492458e+00 ...  2.66101619e-04\n",
      "   1.20797216e-04  9.05639026e-05]\n",
      " [-5.49937622e+02  2.05582409e+01 -1.82497749e+01 ...  2.15354215e-04\n",
      "   1.42610632e-04  5.24237585e-05]\n",
      " [-6.91638672e+02  7.27555847e+01  5.44958353e+00 ...  1.47388891e-05\n",
      "   6.60104979e-06  2.86574937e-06]]\n"
     ]
    }
   ],
   "source": [
    "print(\"train\\n\", x_train, \"\\ntest\\n\", x_test)\n",
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 180\n"
     ]
    }
   ],
   "source": [
    " # 7. And get the number of features extracted.\n",
    "print(f'Features: {x_train.shape[1]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Now, let’s initialize an MLPClassifier. This is a Multi-layer Perceptron Classifier; it optimizes the log-loss function using LBFGS or stochastic gradient descent. Unlike SVM or Naive Bayes, the MLPClassifier has an internal neural network for the purpose of classification. This is a feedforward ANN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataFlair - Initialize the Multi Layer Perceptron Classifier\n",
    "model=MLPClassifier(alpha=0.01,\n",
    "                    batch_size=256, \n",
    "                    epsilon=1e-08,\n",
    "                    hidden_layer_sizes=(300,),\n",
    "                    learning_rate='adaptive', \n",
    "                    max_iter=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(alpha=0.01, batch_size=256, hidden_layer_sizes=(300,),\n",
       "              learning_rate='adaptive', max_iter=500)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #DataFlair - Predict for the test set\n",
    "y_pred=model.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. To calculate the accuracy of our model, we’ll call up the accuracy_score() function we imported from sklearn. Finally, we’ll round the accuracy to 2 decimal places and print it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 48.06%\n"
     ]
    }
   ],
   "source": [
    "#DataFlair - Calculate the accuracy of our model\n",
    "accuracy=accuracy_score(y_true=y_test, y_pred=y_pred)\n",
    "\n",
    "#DataFlair - Print the accuracy\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions[file_name.split(\"-\")[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
